[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Why Real-World Representation in Machine Learning Datasets Matters",
    "section": "",
    "text": "This chart illustrates the improved diversity in the dataset after expansion, ensuring better model performance on varied real-world scenarios.\n\n\nIn machine learning, the adage ‚Äúgarbage in, garbage out‚Äù rings true. A model‚Äôs ability to generalize and perform well hinges on the quality of the data it‚Äôs trained on. Recently, I worked on a project predicting machine learning runtime based on system specifications and dataset properties‚Äîa seemingly straightforward task that taught me a profound lesson: true representation in datasets is key to building robust models.\nIn this article, I‚Äôll walk you through my journey, from the initial excitement to the challenges of ensuring real-world representation, and share actionable insights along the way.\n\n\nWhat Was the Goal?\nThe objective was to create a machine learning model capable of predicting how long a training session would take, given specific system specs and dataset properties. This is a practical problem for teams managing computational resources and optimizing training workflows.\nAt first, the dataset appeared comprehensive: thousands of rows with varying system configurations, dataset sizes, and algorithm complexities. The model trained and validated well within this dataset. But then, the cracks started to show.\n\n\n\nThe Problem: A Lack of Generalization\n\n\n\nPredicted vs.¬†actual runtime: Highlighting how poor dataset representation leads to systematic errors in outlier predictions.\n\n\nWhen tested on real-world setups where training times ranged from seconds to days, the model‚Äôs predictions fell apart. Why? Because the dataset didn‚Äôt fully represent the breadth of real-world scenarios:\n\nSystem diversity was limited: Most data points were from mid-range setups, neglecting low-end and high-end systems.\nTraining durations were narrow: Many samples captured ‚Äúaverage‚Äù training times but missed edge cases like extremely short or prolonged sessions.\n\nThis lack of diversity in the dataset meant the model was essentially ‚Äúoverfitted‚Äù to a narrow slice of reality.\n\n\n\nWhat Did I Do to Fix It?\n\nStep 1: Expanding Dataset Diversity\nTo improve representation, I expanded the dataset to include:\n\nLow-end systems with limited computational resources.\nHigh-end systems with advanced hardware specs.\nTraining durations spanning from seconds to days.\n\n\n\nSystem Diversity\nThe first step in the expansion was addressing the imbalance in system specifications. Many data points were clustered around mid-range systems, leaving low-end and high-end systems underrepresented. By including a broader range of system configurations, we achieved a more balanced dataset.\n\n\n\nImproved representation: This scatter plot shows the relationship between system specifications and training durations, capturing the diversity added during dataset expansion.\n\n\n\n\nTraining Duration Diversity\nIn addition to system diversity, I expanded the range of training durations. The original dataset primarily focused on average training times, which neglected edge cases like extremely short or prolonged sessions. The inclusion of mid-range and long durations improved the model‚Äôs ability to generalize.\n\n\n\nThis boxplot highlights the improved diversity in training durations after dataset expansion, ensuring better representation of real-world scenarios.\n\n\nThis expansion brought much-needed variety, but it also introduced new challenges. For instance, the additional data created noise, causing the model‚Äôs performance to temporarily decline. This was my first lesson: balancing dataset diversity and noise is an art.\n\n\n\n\nKey Lessons Learned\n\n\n\nAn infographic summarizing the three lessons‚Äîrepresentation, balance, and iteration.\n\n\nHere are the core takeaways from my experience:\n\nReal-World Representation is Critical\nA model‚Äôs success depends on how well its training dataset mirrors the scenarios it will encounter in production. Without this alignment, even a high-accuracy model can fail spectacularly.\nBalancing Diversity is a Delicate Process\nSimply adding more data isn‚Äôt enough. Dataset diversity needs to be carefully curated to ensure the model learns meaningful patterns without being overwhelmed by noise.\nIteration is Essential\nImproving dataset representation is not a one-time fix. It requires continuous refinement and evaluation to align the model with real-world complexities.\n\n\n\n\nBroader Implications\n\n\n\nA flowchart showing how dataset diversity affects predictions across domains like healthcare, NLP, and finance.\n\n\nThis issue extends far beyond my project. Here are some real-world examples where dataset representation matters:\n\nHealthcare: Predictive models trained on non-representative patient demographics may lead to biased medical decisions.\nNatural Language Processing: Chatbots trained on limited dialects or languages struggle to serve diverse populations.\nFinance: Risk models trained on historical data can perpetuate systemic biases if certain groups are underrepresented.\n\n\n\n\nTools and Techniques for Better Representation\n\n\n\nA visual guide to tools like SMOTE, EDA techniques, and validation strategies.\n\n\nIf you‚Äôre grappling with similar challenges, here are some strategies to improve dataset representation:\n\nUse EDA to Identify Gaps\nVisualize your data to spot underrepresented groups or scenarios.\nLeverage Synthetic Data\nTools like SMOTE can help augment data in underrepresented categories.\nValidate on Diverse Scenarios\nTest your model on edge cases and real-world setups to catch weaknesses early.\nIterate and Refine\nTreat data preparation as an ongoing process, not a one-off task.\n\n\n\n\nWhy This Matters\nPredicting ML runtime isn‚Äôt just an academic exercise; it‚Äôs a real-world problem with practical stakes:\n\nEfficient Resource Allocation: Accurate predictions help teams optimize hardware usage and reduce costs.\nRealistic Expectations: Stakeholders can plan timelines effectively, avoiding unnecessary delays or inefficiencies.\n\n\n\n\nA workflow diagram showing how better predictions impact project timelines and resource management.\n\n\nWithout a dataset that truly reflects the problem space, predictions can lead to wasted resources or poor decision-making.\n\n\n\nFinal Thoughts\nThis project reinforced the critical role of data diversity and quality in machine learning. A well-represented dataset doesn‚Äôt just make your model work; it makes it reliable, robust, and ready for real-world scenarios.\nHave you faced challenges with dataset representation in your projects? Let‚Äôs discuss! Share your experiences and strategies in the comments below‚ÄîI‚Äôd love to learn from your insights. üöÄ\n\n\n\nWhat‚Äôs Next?\nStay tuned as I continue exploring the intersection of machine learning and real-world challenges. Let‚Äôs build better, more inclusive models together!"
  }
]